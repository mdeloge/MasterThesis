{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import urllib2\n",
    "import requests\n",
    "from geopy.geocoders import Nominatim\n",
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving historic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 99/752 [00:00<00:02, 323.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darksky_full_1430172000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 752/752 [00:02<00:00, 329.69it/s]\n"
     ]
    }
   ],
   "source": [
    "directory = 'blob/full_data/'\n",
    "destination = 'blob/historic/'\n",
    "dirs = os.listdir(directory)\n",
    "\n",
    "for fl in tqdm(dirs):\n",
    "    try:\n",
    "        dest = open(destination + 'darksky_historic_' + fl.split('_')[2], 'w+')\n",
    "        src = open(directory + fl, 'r')\n",
    "        json_src = json.load(src)\n",
    "        #dest.write(json.dumps(json_src[\"daily\"], indent=4))\n",
    "        dest.write(json.dumps(json_src, indent=4))\n",
    "\n",
    "        src.close()\n",
    "        dest.close()\n",
    "    except:\n",
    "        print fl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/746 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Extra data: line 31 column 2 - line 721 column 2 (char 1148 - 27697)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-50e6401aeac2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mjson_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#dest.write(json.dumps(json_src[\"daily\"], indent=4))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matteus/anaconda2/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0mparse_constant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_constant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_pairs_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m         **kw)\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matteus/anaconda2/lib/python2.7/json/__init__.pyc\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/matteus/anaconda2/lib/python2.7/json/decoder.pyc\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrmsg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Extra data: line 31 column 2 - line 721 column 2 (char 1148 - 27697)"
     ]
    }
   ],
   "source": [
    "directory = 'blob/parsed_data/'\n",
    "destination = 'blob/'\n",
    "dirs = os.listdir(directory)\n",
    "dest = open(destination + 'darksky_combined_1.json', 'w+')\n",
    "counter = 0\n",
    "\n",
    "for fl in tqdm(dirs):\n",
    "    src = open(directory + fl, 'r')\n",
    "    json_src = json.load(src)\n",
    "    #dest.write(json.dumps(json_src[\"daily\"], indent=4))\n",
    "    dest.write(json.dumps(json_src, indent=4) + \"\\n\")\n",
    "\n",
    "    src.close()\n",
    "    counter += 1\n",
    "\n",
    "    if(counter % 500 == 0):\n",
    "        dest.close()\n",
    "        dest = open(destination + 'darksky_combined_' + str(counter/500 + 1) + '.json' , 'w+')\n",
    "        \n",
    "   \n",
    "dest.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_t_c(fahrenheit):\n",
    "    return (((fahrenheit - 32) * 5.0) / 9.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 33/747 [00:00<00:04, 156.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1484694000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 155/747 [00:00<00:03, 171.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1478818800.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 296/747 [00:01<00:02, 168.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1441231200.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 384/747 [00:02<00:02, 169.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1439676000.json\n",
      "'pressure'\\Error found in file darksky_historic_1447714800.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 436/747 [00:02<00:01, 159.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1455750000.json\n",
      "'pressure'\\Error found in file darksky_historic_1478732400.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 503/747 [00:03<00:01, 157.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1483052400.json\n",
      "Combined folder found :p\n",
      "'pressure'\\Error found in file darksky_historic_1430604000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 689/747 [00:04<00:00, 134.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pressure'\\Error found in file darksky_historic_1459548000.json\n",
      "'pressure'\\Error found in file darksky_historic_1484262000.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:04<00:00, 156.54it/s]\n"
     ]
    }
   ],
   "source": [
    "directory = 'blob/historic/'\n",
    "destination = 'blob/parsed_data/'\n",
    "dirs = os.listdir(directory)\n",
    "\n",
    "for fl in tqdm(dirs):\n",
    "    try:\n",
    "        src = open(directory + fl, 'r')\n",
    "        dest = open(destination + 'darksky_parsed_' + fl.split(\"_\")[2], 'w+')\n",
    "        json_src = json.load(src)  \n",
    "        json_temp = json_src.get('hourly').get('data')\n",
    "\n",
    "\n",
    "        for temp in json_temp:\n",
    "            data = {}\n",
    "            #changing hourly keys\n",
    "            data[\"HourlyTimestamp\"] = dt.datetime.fromtimestamp(temp[\"time\"]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            data[\"HourlyTemperature\"] = f_t_c(temp[\"temperature\"])\n",
    "            data[\"HourlyDewPoint\"] = temp[\"dewPoint\"]\n",
    "            data[\"HourlySummary\"] = temp[\"summary\"]\n",
    "            data[\"HourlyApparentTemperature\"] = f_t_c(temp[\"apparentTemperature\"])\n",
    "            data[\"HourlyPressure\"] = temp[\"pressure\"]\n",
    "            data[\"HourlyWindSpeed\"] = temp[\"windSpeed\"]            \n",
    "            data[\"HourlyWindBearing\"] = temp[\"windBearing\"]\n",
    "            data[\"HourlyHumidity\"] = temp[\"humidity\"]\n",
    "            data[\"HourlyIcon\"] = temp[\"icon\"]\n",
    "\n",
    "            #appending daily data\n",
    "            data[\"DailyTemperatureMax\"] = f_t_c(json_src.get(\"daily\").get('data')[0][\"temperatureMax\"])\n",
    "            data[\"DailyApparentTemperatureMinTime\"] = json_src.get(\"daily\").get('data')[0][\"apparentTemperatureMinTime\"]\n",
    "            data[\"DailyTemperatureMin\"] = f_t_c(json_src.get(\"daily\").get('data')[0][\"temperatureMin\"])\n",
    "            data[\"DailySummary\"] = json_src.get(\"daily\").get('data')[0][\"summary\"]\n",
    "            data[\"DailyDewPoint\"] = json_src.get(\"daily\").get('data')[0][\"dewPoint\"]\n",
    "            data[\"DailyApparentTemperatureMax\"] = f_t_c(json_src.get(\"daily\").get('data')[0][\"apparentTemperatureMax\"])\n",
    "            data[\"DailyTemperatureMaxTime\"] = json_src.get(\"daily\").get('data')[0][\"temperatureMaxTime\"]\n",
    "            data[\"DailyWindBearing\"] = json_src.get(\"daily\").get('data')[0][\"windBearing\"]\n",
    "            data[\"DailyMoonPhase\"] = json_src.get(\"daily\").get('data')[0][\"moonPhase\"]\n",
    "            data[\"DailySunsetTime\"] = json_src.get(\"daily\").get('data')[0][\"sunsetTime\"]\n",
    "            data[\"DailyPressure\"] = json_src.get(\"daily\").get('data')[0][\"pressure\"]\n",
    "            data[\"DailyApparentTemperatureMin\"] = f_t_c(json_src.get(\"daily\").get('data')[0][\"apparentTemperatureMin\"])\n",
    "            data[\"DailyIcon\"] = json_src.get(\"daily\").get('data')[0][\"icon\"]\n",
    "            data[\"DailyapparentTemperatureMaxTime\"] = json_src.get(\"daily\").get('data')[0][\"apparentTemperatureMaxTime\"]\n",
    "            data[\"DailyHumidity\"] = json_src.get(\"daily\").get('data')[0][\"humidity\"]\n",
    "            data[\"DailyWindSpeed\"] = json_src.get(\"daily\").get('data')[0][\"windSpeed\"]\n",
    "            data[\"DailyDate\"] = dt.datetime.fromtimestamp(json_src.get(\"daily\").get('data')[0][\"time\"]).strftime('%Y-%m-%d %H:%M:%S')\n",
    "            data[\"DailySunriseTime\"] = json_src.get(\"daily\").get('data')[0][\"sunriseTime\"]\n",
    "            data[\"DailyTemperatureMinTime\"] = json_src.get(\"daily\").get('data')[0][\"temperatureMinTime\"]\n",
    "\n",
    "            dest.write(json.dumps(data, indent=4) + \"\\n\")\n",
    "\n",
    "    except KeyError as e:\n",
    "        print str(e) + \"\\Error found in file \" + fl\n",
    "    except IOError as ioe:\n",
    "        print \"Combined folder found :p\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #dest.write(json.dumps(json_src[\"daily\"], indent=4))\n",
    "\n",
    "\n",
    "    src.close()\n",
    "    dest.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
